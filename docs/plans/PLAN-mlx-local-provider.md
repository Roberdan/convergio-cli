# Piano: MLX Local Provider per Convergio

## Obiettivo
Aggiungere supporto completo per esecuzione locale via MLX (Apple Silicon), permettendo a Convergio di funzionare **100% offline** senza dipendenze esterne.

## Architettura Target

```
Convergio
‚îú‚îÄ‚îÄ Provider: Anthropic (cloud) - default
‚îú‚îÄ‚îÄ Provider: OpenAI (cloud)
‚îú‚îÄ‚îÄ Provider: Gemini (cloud)
‚îú‚îÄ‚îÄ Provider: OpenRouter (cloud)
‚îÇ
‚îî‚îÄ‚îÄ Provider: MLX (locale)           ‚Üê NUOVO
        ‚îú‚îÄ‚îÄ Embeddings: MiniLM (~80MB)
        ‚îî‚îÄ‚îÄ LLM:
            ‚îú‚îÄ‚îÄ Llama 3.2 1B (~1.5GB) - velocissimo
            ‚îú‚îÄ‚îÄ Llama 3.2 3B (~3GB)   - balanced
            ‚îú‚îÄ‚îÄ Phi-3 Mini (~2.5GB)   - compatto
            ‚îú‚îÄ‚îÄ Mistral 7B Q4 (~4.5GB)- potente
            ‚îî‚îÄ‚îÄ Llama 3.1 8B Q4 (~5GB)- best quality
```

---

## Gestione Aggiornamento Modelli

### Problema
I modelli evolvono (nuovi release, deprecazioni, cambi capacit√†). Come aggiornare senza rilasciare nuova versione?

### Soluzione: Registry Ibrido

```c
// 1. Embedded registry (compilato nel binario)
static const ModelInfo EMBEDDED_MODELS[] = {
    {"llama-3.2-1b", "Llama 3.2 1B", 1500, 131072, true, "2024-12-01"},
    ...
};

// 2. Remote registry (opzionale, scaricato periodicamente)
// https://raw.githubusercontent.com/Roberdan/convergio-cli/main/config/models-registry.json
{
  "version": 2,
  "updated": "2025-01-15",
  "models": [
    {"id": "llama-3.2-1b", "deprecated": false, "recommended": true},
    {"id": "llama-3.3-1b", "new": true, "replaces": "llama-3.2-1b"}  // NEW!
  ]
}

// 3. Logica merge
ModelInfo* get_available_models() {
    ModelInfo* local = load_embedded_registry();
    ModelInfo* remote = fetch_remote_registry();  // con cache 24h
    return merge_registries(local, remote);       // remote ha priorit√†
}
```

### Comportamento
- **Offline**: usa solo embedded registry
- **Online**: check remote ogni 24h, merge con embedded
- **Notifica**: "Nuovo modello disponibile: Llama 3.3" nel banner

---

## Work Packages (Parallelizzabili)

### WP1: Core MLX Provider [~600 righe]
**Owner**: Thread A
**File**: `src/providers/mlx.m`, `include/nous/mlx.h`

```
‚îú‚îÄ‚îÄ mlx_provider_create()      - Factory
‚îú‚îÄ‚îÄ mlx_init()                 - Carica modello
‚îú‚îÄ‚îÄ mlx_shutdown()             - Cleanup
‚îú‚îÄ‚îÄ mlx_chat()                 - Inference LLM
‚îú‚îÄ‚îÄ mlx_chat_stream()          - Streaming
‚îú‚îÄ‚îÄ mlx_embed_text()           - Embeddings
‚îú‚îÄ‚îÄ mlx_get_model_info()       - Info modello attivo
‚îî‚îÄ‚îÄ mlx_supports_tools()       - Check tool calling
```

**Dipendenze**:
- Metal.framework (gi√† linkato)
- Accelerate.framework (gi√† linkato)
- MLX C bindings

### WP2: Model Manager [~400 righe]
**Owner**: Thread B
**File**: `src/models/model_manager.c`, `include/nous/model_manager.h`

```
‚îú‚îÄ‚îÄ model_list_available()     - Lista modelli (embedded + remote)
‚îú‚îÄ‚îÄ model_download()           - Download con progress bar
‚îú‚îÄ‚îÄ model_delete()             - Rimuovi modello
‚îú‚îÄ‚îÄ model_get_path()           - Path locale
‚îú‚îÄ‚îÄ model_verify_integrity()   - Check SHA256
‚îî‚îÄ‚îÄ model_get_size()           - Size in bytes
```

**Storage**:
```
~/.convergio/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ llama-3.2-1b/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ minilm-l6-v2/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ models-cache.json          - Cache registry remoto
‚îî‚îÄ‚îÄ config.json                - Preferenze utente
```

### WP3: Setup Wizard Enhancement [~500 righe]
**Owner**: Thread C
**File**: `src/core/commands/setup_wizard.c`

Estendi il wizard esistente con:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONVERGIO SETUP                                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1) API Keys           - Configure cloud providers              ‚îÇ
‚îÇ  2) Agent Models       - Set models per agent                   ‚îÇ
‚îÇ  3) Local Models       - Download & manage MLX models    ‚Üê NEW  ‚îÇ
‚îÇ  4) Embeddings         - Configure semantic search       ‚Üê NEW  ‚îÇ
‚îÇ  5) View Configuration                                          ‚îÇ
‚îÇ  6) Exit                                                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Menu "Local Models"
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LOCAL MODELS (MLX)                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Run AI models directly on your Mac without cloud APIs.         ‚îÇ
‚îÇ  Requires Apple Silicon (M1/M2/M3/M4).                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  INSTALLED MODELS:                                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  ‚úì Llama 3.2 3B        3.1 GB    Default                        ‚îÇ
‚îÇ  ‚úì MiniLM-L6 (embed)   80 MB     For semantic search            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  AVAILABLE TO DOWNLOAD:                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  1) Llama 3.2 1B       1.5 GB    Fastest, basic tasks           ‚îÇ
‚îÇ  2) Phi-3 Mini         2.5 GB    Compact, good reasoning        ‚îÇ
‚îÇ  3) Mistral 7B Q4      4.5 GB    Powerful, needs 16GB RAM       ‚îÇ
‚îÇ  4) Llama 3.1 8B Q4    5.0 GB    Best quality, needs 16GB RAM   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Actions:                                                       ‚îÇ
‚îÇ    D) Download a model                                          ‚îÇ
‚îÇ    R) Remove installed model                                    ‚îÇ
‚îÇ    S) Set default model                                         ‚îÇ
‚îÇ    B) Back                                                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Download con Progress Bar
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DOWNLOADING: Llama 3.2 3B                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  62%                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Downloaded:  1.92 GB / 3.10 GB                                 ‚îÇ
‚îÇ  Speed:       45.2 MB/s                                         ‚îÇ
‚îÇ  ETA:         26 seconds                                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Press Ctrl+C to cancel                                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### WP4: Dynamic Context Limits [~200 righe]
**Owner**: Thread D
**File**: `src/context/compaction.c`, `include/nous/compaction.h`

Modifica `COMPACTION_THRESHOLD_TOKENS` da costante a funzione:

```c
// Prima (statico)
#define COMPACTION_THRESHOLD_TOKENS 80000

// Dopo (dinamico)
size_t compaction_get_threshold(void) {
    ModelInfo* model = provider_get_current_model();
    if (!model) return 80000;  // default

    // Use 75% of context window for safety
    return (size_t)(model->context_window * 0.75);
}

// Esempi:
// Claude Sonnet 4.5: 200K * 0.75 = 150K threshold
// Llama 3.2 3B:      128K * 0.75 = 96K threshold
// Mistral 7B:        32K * 0.75 = 24K threshold
```

### WP5: Provider Registry Update [~150 righe]
**Owner**: Thread E
**File**: `src/providers/provider.c`, `include/nous/provider.h`

```c
typedef enum {
    PROVIDER_ANTHROPIC   = 0,
    PROVIDER_OPENAI      = 1,
    PROVIDER_GEMINI      = 2,
    PROVIDER_OPENROUTER  = 3,
    PROVIDER_OLLAMA      = 4,
    PROVIDER_MLX         = 5,   // NEW
    PROVIDER_COUNT       = 6
} ProviderType;

// Model capability flags
typedef struct {
    bool supports_tools;
    bool supports_vision;
    bool supports_streaming;
    size_t context_window;
    size_t max_output_tokens;
} ModelCapabilities;
```

### WP6: Model Registry [~300 righe]
**Owner**: Thread F
**File**: `src/models/registry.c`, `config/models-registry.json`

```c
typedef struct {
    const char* id;              // "llama-3.2-3b"
    const char* display_name;    // "Llama 3.2 3B"
    const char* huggingface_id;  // "mlx-community/Llama-3.2-3B-Instruct-4bit"
    size_t size_mb;              // 3100
    size_t context_window;       // 131072
    size_t min_ram_gb;           // 8
    bool supports_tools;         // true
    bool supports_italian;       // true (for multilingual)
    const char* best_for;        // "general, coding, reasoning"
    const char* sha256;          // for integrity verification
} LocalModelInfo;
```

### WP7: Help & Documentation [~400 righe]
**Owner**: Thread G
**File**: `src/core/commands/commands.c` (help), `README.md`, `docs/ADR-*.md`

#### Nuove sezioni help:
```
/help local    - Guida modelli locali MLX
/help setup    - Guida wizard configurazione
/help models   - Lista modelli disponibili (cloud + local)
```

#### Contenuto /help local:
```
üè† LOCAL MODELS (MLX)

Run AI models directly on your Mac without cloud APIs or internet.
Requires Apple Silicon (M1/M2/M3/M4).

QUICK START:
  /setup ‚Üí Local Models ‚Üí Download a model

COMMANDS:
  /setup local              Open local models configuration
  /models                   List all available models
  /model <name>             Switch to a specific model
  /model info               Show current model details

BENEFITS:
  ‚úì 100% offline operation
  ‚úì Complete privacy (data never leaves your Mac)
  ‚úì No API costs
  ‚úì Low latency (no network roundtrip)

LIMITATIONS:
  ‚Ä¢ Requires model download (1-5 GB per model)
  ‚Ä¢ Quality varies (local < Claude for complex tasks)
  ‚Ä¢ RAM requirements (8GB minimum, 16GB+ recommended)
  ‚Ä¢ Tool calling less reliable than Claude

RECOMMENDED MODELS:
  ‚Ä¢ Llama 3.2 3B   - Best balance of speed and quality
  ‚Ä¢ Mistral 7B    - For coding and reasoning tasks
  ‚Ä¢ Llama 3.1 8B  - Highest quality (needs 16GB RAM)

See: https://github.com/Roberdan/convergio-cli#local-models
```

### WP8: CLI Arguments & Config [~200 righe]
**Owner**: Thread H
**File**: `src/core/main.c`, `src/core/config.c`

```bash
# Nuovi argomenti CLI
convergio --provider mlx                    # Usa MLX provider
convergio --provider mlx --model llama-3b   # Specifica modello
convergio --offline                         # Forza modalit√† offline
convergio --list-models                     # Lista modelli installati

# Config file (~/.convergio/config.json)
{
  "default_provider": "anthropic",
  "local": {
    "enabled": true,
    "default_model": "llama-3.2-3b",
    "embedding_model": "minilm-l6-v2",
    "auto_download": false
  },
  "embeddings": {
    "provider": "openai",           // o "mlx" per locale
    "model": "text-embedding-3-small"
  }
}
```

---

## Timeline Parallela

```
Week 1:
‚îú‚îÄ‚îÄ WP1 Core MLX Provider      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 1-5
‚îú‚îÄ‚îÄ WP2 Model Manager          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 1-5
‚îú‚îÄ‚îÄ WP5 Provider Registry      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 1-2
‚îî‚îÄ‚îÄ WP6 Model Registry         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 1-2

Week 2:
‚îú‚îÄ‚îÄ WP3 Setup Wizard           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Day 3-7
‚îú‚îÄ‚îÄ WP4 Dynamic Context        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 3-4
‚îú‚îÄ‚îÄ WP7 Documentation          ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Day 5-7
‚îî‚îÄ‚îÄ WP8 CLI & Config           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Day 4-5

Testing & Polish:
‚îî‚îÄ‚îÄ Integration Testing        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Day 7-8
```

---

## Dipendenze tra WP

```
WP5 (Provider Registry) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí WP1 (MLX Provider)
                             ‚îÇ
WP6 (Model Registry)   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí WP2 (Model Manager)
                             ‚îÇ
                             ‚îî‚îÄ‚îÄ‚Üí WP3 (Setup Wizard)

WP1 (MLX Provider) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí WP4 (Dynamic Context)

WP1 + WP2 + WP3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí WP7 (Documentation)

WP1 + WP2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí WP8 (CLI & Config)
```

---

## File da Creare

| File | Righe | WP |
|------|-------|-----|
| `src/providers/mlx.m` | ~500 | WP1 |
| `include/nous/mlx.h` | ~100 | WP1 |
| `src/models/model_manager.c` | ~400 | WP2 |
| `include/nous/model_manager.h` | ~80 | WP2 |
| `src/models/registry.c` | ~300 | WP6 |
| `include/nous/model_registry.h` | ~60 | WP6 |
| `config/models-registry.json` | ~100 | WP6 |
| `docs/ADR-mlx-local-provider.md` | ~150 | WP7 |

## File da Modificare

| File | Modifiche | WP |
|------|-----------|-----|
| `src/core/commands/setup_wizard.c` | +400 righe | WP3 |
| `src/context/compaction.c` | ~50 righe | WP4 |
| `include/nous/compaction.h` | ~20 righe | WP4 |
| `src/providers/provider.c` | ~100 righe | WP5 |
| `include/nous/provider.h` | ~50 righe | WP5 |
| `src/core/main.c` | ~100 righe | WP8 |
| `src/core/config.c` | ~100 righe | WP8 |
| `src/core/commands/commands.c` | ~200 righe (help) | WP7 |
| `README.md` | ~300 righe | WP7 |
| `Makefile` | ~30 righe | WP1 |

---

## Stima Totale

| Categoria | Righe |
|-----------|-------|
| Nuovi file | ~1540 |
| Modifiche | ~1350 |
| **TOTALE** | **~2900 righe** |

---

## Test Plan

### Unit Tests
- `tests/test_mlx_provider.c` - MLX inference tests
- `tests/test_model_manager.c` - Download/delete tests
- `tests/test_registry.c` - Registry merge tests

### Integration Tests
- Full conversation with local model
- Tool calling with Mistral/Llama
- Embedding + search flow
- Context compaction with dynamic limits

### Manual Tests
- Setup wizard complete flow
- Download interruption + resume
- Offline mode verification
- Model switching mid-conversation

---

## Rischi e Mitigazioni

| Rischio | Probabilit√† | Impatto | Mitigazione |
|---------|-------------|---------|-------------|
| MLX API cambia | Media | Alto | Pin versione, abstract layer |
| Modelli deprecati | Alta | Medio | Registry remoto con fallback |
| RAM insufficiente | Media | Alto | Check pre-download, warning |
| Download fallisce | Media | Medio | Resume, retry, checksum |
| Tool calling fallisce | Alta | Medio | Retry logic, fallback cloud |

---

## Decisioni Aperte

1. **Dove hostare i modelli?**
   - Opzione A: HuggingFace (gi√† l√¨, ma rate limits)
   - Opzione B: GitHub Releases (nostro controllo)
   - Opzione C: CDN dedicato (costi)

   **Raccomandazione**: HuggingFace per ora, con mirror su GitHub Releases per modelli critici.

2. **Tokenizer?**
   - Opzione A: SentencePiece (C library, +dipendenza)
   - Opzione B: Tiktoken (Python, no)
   - Opzione C: Tokenizer in Swift/Obj-C (pi√π lavoro)

   **Raccomandazione**: SentencePiece come libreria statica linkate nel binario.

3. **Quantizzazione default?**
   - 4-bit (pi√π piccolo, leggermente peggiore)
   - 8-bit (pi√π grande, migliore qualit√†)

   **Raccomandazione**: 4-bit per default, 8-bit disponibile per chi ha RAM.

---

## Prossimi Passi

### Completati (13/12/2025)

1. ‚úÖ Merge main ‚Üí semantic-graph (fatto)

2. ‚úÖ **WP5: Provider Registry Update** (completato)
   - Aggiunto PROVIDER_MLX enum in provider.h
   - Aggiunto mlx_provider_create() declaration
   - Aggiunto array g_mlx_models[] con 10 modelli (incluso DeepSeek R1)
   - Aggiornate funzioni model_get_config/model_get_by_provider

3. ‚úÖ **WP1: Core MLX Provider** (~600 righe, completato)
   - Creato include/nous/mlx.h con API completa
   - Implementato src/providers/mlx.m con:
     - 10 modelli MLX (Llama, DeepSeek R1, Qwen, Phi-3, Mistral)
     - Hardware detection (Apple Silicon)
     - Model loading/unloading
     - Generate (placeholder per MLX bindings)
     - Provider factory

4. ‚úÖ **WP2: Model Manager** (~400 righe, completato)
   - Funzioni download_mlx_model() in setup_wizard.c
   - Funzioni delete_mlx_model() in setup_wizard.c
   - Integrazione con HuggingFace mlx-community

5. ‚úÖ **WP3: Setup Wizard Enhancement** (~500 righe, completato)
   - Aggiunto menu "Local Models" nel wizard principale
   - Lista modelli con status (downloaded/not downloaded)
   - Download interattivo con progress
   - Delete modelli installati
   - Help per MLX provider

6. ‚úÖ **WP6: Model Registry** (~300 righe, completato)
   - Aggiunta sezione "mlx" in config/models.json
   - 9 modelli MLX con metadata completi
   - size_mb, min_ram_gb, best_for per ogni modello

7. ‚úÖ **WP4: Dynamic Context Limits** (~200 righe, completato)
   - Aggiunto g_dynamic_threshold in compaction.c
   - compaction_set_dynamic_threshold() basato su context window
   - compaction_get_threshold() e compaction_reset_threshold()
   - Costanti COMPACTION_THRESHOLD_RATIO, MIN, MAX

8. ‚úÖ **WP7: Help & Documentation** (completato)
   - ‚úÖ Help --help aggiornato con opzioni MLX
   - ‚úÖ /help local command aggiunto
   - ‚úÖ ADR-mlx-local-provider.md creato

9. ‚úÖ **WP8: CLI Arguments & Config** (completato)
   - ‚úÖ --local, --model flags aggiunti in main.c
   - ‚úÖ g_use_local_mlx e g_mlx_model variabili globali

10. ‚úÖ **ADR**: Creato docs/ADR-mlx-local-provider.md

### MLX-Swift Integration (13/12/2025)

**Major Update**: Implemented real MLX inference via mlx-swift instead of placeholder code.

1. ‚úÖ **Package.swift** created with mlx-swift-lm dependencies
   - mlx-swift-lm (main branch) for MLXLLM, MLXLMCommon
   - Static library build (.a)

2. ‚úÖ **Sources/ConvergioMLX/MLXBridge.swift** created
   - @_cdecl exported C functions for ObjC interop
   - mlx_bridge_is_available() - Check Apple Silicon + macOS 14+
   - mlx_bridge_load_model() - Load from HuggingFace
   - mlx_bridge_generate() - LLM inference
   - mlx_bridge_unload_model() - Memory cleanup

3. ‚úÖ **Makefile** updated for Swift compilation
   - Swift Package Manager build step
   - C++ runtime linking (-lc++)
   - Swift toolchain compatibility libs

4. ‚úÖ **mlx.m** updated to call Swift bridge
   - No more placeholder code
   - Real inference via mlx_bridge_generate()

5. ‚úÖ **Build succeeds**: 32MB binary with MLX support

### Da Fare

11. ‚¨ú Testing integrato (model download + inference test)
12. ‚¨ú PR review

---

## Progress Summary

| WP | Nome | Status | Completamento |
|----|------|--------|---------------|
| WP1 | Core MLX Provider | ‚úÖ | 100% |
| WP2 | Model Manager | ‚úÖ | 100% |
| WP3 | Setup Wizard | ‚úÖ | 100% |
| WP4 | Dynamic Context | ‚úÖ | 100% |
| WP5 | Provider Registry | ‚úÖ | 100% |
| WP6 | Model Registry | ‚úÖ | 100% |
| WP7 | Documentation | ‚úÖ | 100% |
| WP8 | CLI Arguments | ‚úÖ | 100% |
| ADR | Architecture Doc | ‚úÖ | 100% |

**Totale: 100% completato** (solo testing e PR review rimasti)
